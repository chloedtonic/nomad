{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelIO\n",
    "\n",
    "> 해당 자료 참고 링크 : [langchain v0.1 기준 Docs 페이지링크 : 현재 유지보수 지원하지 않는 버전](https://python.langchain.com/v0.1/docs/modules/)\n",
    "\n",
    "- prompt\n",
    "- chat models\n",
    "- llms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FewshotPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미국 수도는?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 기본 방식\n",
    "t = PromptTemplate(\n",
    "    template=\"{country} 수도는?\",\n",
    "    input_variables = [\"country\"]\n",
    "    )\n",
    "\n",
    "t = PromptTemplate.from_template(\n",
    "    \"{country} 수도는?\",\n",
    "    )\n",
    "t.format(country = \"미국\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"food\": \"파전\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:FewShotPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"food\": \"파전\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:FewShotPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"base\",\n",
      "    \"StringPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"text\": \"Human : 된장찌개 설명해줘\\nAI:\\n    나라 : 한국\\n    주요 재료 : 된장, 두부, 호박\\n    비슷한 음식 : 청국장\\n    \\n\\nHuman : 김치찌개\\nAI:\\n    나라 : 한국\\n    주요 재료 : 김치, 돼지고기, 양파 \\n    비슷한 음식 : 고추장찌개\\n    \\n\\nHuman : 훠거 설명해줘\\nAI:\\n    나라 : 중국\\n    주요 재료 : 마라, 건두부, 목이버섯\\n    비슷한 음식 : 마라탕\\n    \\n\\nHuman : 파전를 아니\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Human : 된장찌개 설명해줘\\nAI:\\n    나라 : 한국\\n    주요 재료 : 된장, 두부, 호박\\n    비슷한 음식 : 청국장\\n    \\n\\nHuman : 김치찌개\\nAI:\\n    나라 : 한국\\n    주요 재료 : 김치, 돼지고기, 양파 \\n    비슷한 음식 : 고추장찌개\\n    \\n\\nHuman : 훠거 설명해줘\\nAI:\\n    나라 : 중국\\n    주요 재료 : 마라, 건두부, 목이버섯\\n    비슷한 음식 : 마라탕\\n    \\n\\nHuman : 파전를 아니\"\n",
      "  ]\n",
      "}\n",
      "AI:\n",
      "    나라 : 한국\n",
      "    주요 재료 : 부침가루, 파, 해물 (오징어, 새우 등)\n",
      "    비슷한 음식 : 김치전\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] [1.22s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"AI:\\n    나라 : 한국\\n    주요 재료 : 부침가루, 파, 해물 (오징어, 새우 등)\\n    비슷한 음식 : 김치전\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \"AI:\\n    나라 : 한국\\n    주요 재료 : 부침가루, 파, 해물 (오징어, 새우 등)\\n    비슷한 음식 : 김치전\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.22s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI:\\n    나라 : 한국\\n    주요 재료 : 부침가루, 파, 해물 (오징어, 새우 등)\\n    비슷한 음식 : 김치전')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate \n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate # 어떻게 대답해야하는지 LM에게 알려주기 위한 예제를 알려주는 것\n",
    "\n",
    "model=\"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    streaming = True, # LLM response가 생성되는대로 볼 수 있도록 함 /전체 응답이 끝나기 전에 한글자라도 나오면 볼수있음\n",
    "    callbacks = [StreamingStdOutCallbackHandler(),] # 응답을 콘솔로 바로 print 해줌 / callback : 다양한 이벤트들을 감지할 수 있음(llm 시작 종료 등)\n",
    ")\n",
    "# 대화기록같은걸 DB에서 가져와서 예제로 넣어주면 형식화하여 답변을 더 잘할 것\n",
    "# 보고서 형식/문법이라던가 대본 등등\n",
    "\n",
    "# 예제가 없는 경우\n",
    "# chat.predict(\"김치찌개 설명해줘\")\n",
    "\n",
    "examples = [{\n",
    "    \"question\":\"된장찌개 설명해줘\",\n",
    "    \"answer\" : \"\"\"\n",
    "    나라 : 한국\n",
    "    주요 재료 : 된장, 두부, 호박\n",
    "    비슷한 음식 : 청국장\n",
    "    \"\"\"\n",
    "    },{\n",
    "    \"question\":\"김치찌개\",\n",
    "    \"answer\" : \"\"\"\n",
    "    나라 : 한국\n",
    "    주요 재료 : 김치, 돼지고기, 양파 \n",
    "    비슷한 음식 : 고추장찌개\n",
    "    \"\"\"\n",
    "    },\n",
    "    {\n",
    "    \"question\":\"훠거 설명해줘\",\n",
    "    \"answer\" : \"\"\"\n",
    "    나라 : 중국\n",
    "    주요 재료 : 마라, 건두부, 목이버섯\n",
    "    비슷한 음식 : 마라탕\n",
    "    \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# DB에서 가져올수도있고 직접 쓸수도있으므로 형식을 정해줘야함\n",
    "example_tamplate = \"\"\"\n",
    "    Human : {question},\n",
    "    AI : {answer}\n",
    "\"\"\"\n",
    "# example_prompt = PromptTemplate.from_template(example_tamplate) # 답변에 대한 템플릿을 지정해줄 수 있음\n",
    "example_prompt = PromptTemplate.from_template(\"Human : {question}\\nAI:{answer}\") # 답변 형식을 지정해줌\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt =example_prompt, \n",
    "    examples = examples,\n",
    "    suffix  = \"Human : {food}를 아니\" ,# 뒤에 나올 고정말 \n",
    "    input_variables = ['food'] # 유효성 검사 해줌 - prompt.format(food = \"김치찌개\") 에 food 가 없으면 에러남\n",
    "\n",
    ")\n",
    "# 랭체인이 알아서 예제 리스트들을 형식화할것\n",
    "\n",
    "prompt.format(food = \"파전\")\n",
    "\n",
    "chain = prompt | chat \n",
    "\n",
    "chain.invoke({\"food\":\"파전\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"food\": \"파전\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"food\": \"파전\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"너는 요리전문가야, 답변은 이전과 같은 형식으로 짧게 해줘\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"된장찌개에 대해 설명해줘\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"AIMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"\\n    나라 : 한국\\n    주요 재료 : 된장, 두부, 호박\\n    비슷한 음식 : 청국장\\n    \",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"김치찌개에 대해 설명해줘\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"AIMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"\\n    나라 : 한국\\n    주요 재료 : 김치, 돼지고기, 양파 \\n    비슷한 음식 : 고추장찌개\\n    \",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"훠거에 대해 설명해줘\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"AIMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"\\n    나라 : 중국\\n    주요 재료 : 마라, 건두부, 목이버섯\\n    비슷한 음식 : 마라탕\\n    \",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"파전에 대해 설명해줘\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: 너는 요리전문가야, 답변은 이전과 같은 형식으로 짧게 해줘\\nHuman: 된장찌개에 대해 설명해줘\\nAI: \\n    나라 : 한국\\n    주요 재료 : 된장, 두부, 호박\\n    비슷한 음식 : 청국장\\n    \\nHuman: 김치찌개에 대해 설명해줘\\nAI: \\n    나라 : 한국\\n    주요 재료 : 김치, 돼지고기, 양파 \\n    비슷한 음식 : 고추장찌개\\n    \\nHuman: 훠거에 대해 설명해줘\\nAI: \\n    나라 : 중국\\n    주요 재료 : 마라, 건두부, 목이버섯\\n    비슷한 음식 : 마라탕\\n    \\nHuman: 파전에 대해 설명해줘\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "    나라 : 한국\n",
      "    주요 재료 : 부침가루, 파, 해물\n",
      "    비슷한 음식 : 전 (전통 전)\n",
      "    \u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] [1.24s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\n    나라 : 한국\\n    주요 재료 : 부침가루, 파, 해물\\n    비슷한 음식 : 전 (전통 전)\\n    \",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \"\\n    나라 : 한국\\n    주요 재료 : 부침가루, 파, 해물\\n    비슷한 음식 : 전 (전통 전)\\n    \",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.24s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n    나라 : 한국\\n    주요 재료 : 부침가루, 파, 해물\\n    비슷한 음식 : 전 (전통 전)\\n    ')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import (\n",
    "    FewShotPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "example = [\n",
    "    {\n",
    "        \"food\": \"된장찌개\",\n",
    "        \"answer\": \"\"\"\n",
    "    나라 : 한국\n",
    "    주요 재료 : 된장, 두부, 호박\n",
    "    비슷한 음식 : 청국장\n",
    "    \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"food\": \"김치찌개\",\n",
    "        \"answer\": \"\"\"\n",
    "    나라 : 한국\n",
    "    주요 재료 : 김치, 돼지고기, 양파 \n",
    "    비슷한 음식 : 고추장찌개\n",
    "    \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"food\": \"훠거\",\n",
    "        \"answer\": \"\"\"\n",
    "    나라 : 중국\n",
    "    주요 재료 : 마라, 건두부, 목이버섯\n",
    "    비슷한 음식 : 마라탕\n",
    "    \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 형식을 지정해줌\n",
    "example_prompts = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"{food}에 대해 설명해줘\"), (\"ai\", \"{answer}\")]\n",
    ")\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompts,\n",
    "    examples=example\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 요리전문가야, 답변은 이전과 같은 형식으로 짧게 해줘\"),\n",
    "    example_prompt,  # 예제 메시지를 포함\n",
    "    (\"human\", \"{food}에 대해 설명해줘\")  # 새로운 질문\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\"food\": \"파전\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 LengthBasedExampleSelector \n",
    "- 동적으로 예제를 선택할 수 있게 하는 것\n",
    "- 예제를 형식화할 수 있고 , 예제의 양이 얼마나 되는지 알수있음\n",
    "- 설정해놓은 세팅값에 따라 prompt 양을 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "# \"아래 코드에서 food가 없다는 에러가 나는 이유와 코드를 수정해줘줘\"\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "example = [{\n",
    "    \"question\":\"된장찌개 설명해줘\",\n",
    "    \"answer\" : \"\"\"\n",
    "    나라 : 한국\n",
    "    주요 재료 : 된장, 두부, 호박\n",
    "    비슷한 음식 : 청국장\n",
    "    \"\"\"\n",
    "    },{\n",
    "    \"question\":\"김치찌개\",\n",
    "    \"answer\" : \"\"\"\n",
    "    나라 : 한국\n",
    "    주요 재료 : 김치, 돼지고기, 양파 \n",
    "    비슷한 음식 : 고추장찌개\n",
    "    \"\"\"\n",
    "    },\n",
    "    {\n",
    "    \"question\":\"훠거 설명해줘\",\n",
    "    \"answer\" : \"\"\"\n",
    "    나라 : 중국\n",
    "    주요 재료 : 마라, 건두부, 목이버섯\n",
    "    비슷한 음식 : 마라탕\n",
    "    \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompts = PromptTemplate.from_template(\"Human : {question}\\nAI:{answer}\") # 답변 형식을 지정해줌\n",
    "\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=example,\n",
    "    example_prompt=example_prompts,\n",
    "    max_length=50 # 예제의 양을 얼마나 해줄지 설정가능 (예제가 너무 많으면 랭체인이 알아서 줄여줌) / 숫자를 바꿔서 테스트해보기\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt =example_prompts, \n",
    "    example_selector = example_selector, # 이전에는 예제를 다 넣어줬지만 이번에는 예제를 선택해줄 수 있음, 원래 exmple을 넣었었음\n",
    "    suffix  = \"Human : {food} 설명해줘\" ,# 뒤에 나올 고정말 \n",
    "    input_variables = ['food'] # 유효성 검사 해줌 - prompt.format(food = \"김치찌개\") 에 food 가 없으면 에러남\n",
    "\n",
    ")\n",
    "\n",
    "prompt.format(food = \"굴보쌈\")\n",
    "\n",
    "chain = prompt | chat # t사용자의 질문이 LLM 에 전달되어도 예제들이 허용되는 양을 지키면서 LLM에 전달됨\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4.3.2 랜덤 예제 선택\n",
    "    - 나만의 exampel selector를 만들어서 사용하는 방법(원하는대로 예제를 선택할 수 있음)\n",
    "    - 유저의 로그인 여부/유저 사용 언어에 따라 example을 선택할 수 있음\n",
    "    - random example selector 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human : 김치찌개 설명해줘 \\n AI:\\n    나라 : 한국\\n    주요 재료 : 김치, 돼지고기, 양파 \\n    비슷한 음식 : 고추장찌개\\n    \\n\\nHuman : 굴보쌈 설명해줘'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤으로 예제를 선택하게 하는 방식\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [{\n",
    "    \"question\":\"된장찌개 설명해줘\",\n",
    "    \"answer\" : \"\"\"\n",
    "    나라 : 한국\n",
    "    주요 재료 : 된장, 두부, 호박\n",
    "    비슷한 음식 : 청국장\n",
    "    \"\"\"\n",
    "    },{\n",
    "    \"question\":\"김치찌개 설명해줘\",\n",
    "    \"answer\" : \"\"\"\n",
    "    나라 : 한국\n",
    "    주요 재료 : 김치, 돼지고기, 양파 \n",
    "    비슷한 음식 : 고추장찌개\n",
    "    \"\"\"\n",
    "    },\n",
    "    {\n",
    "    \"question\":\"훠거 설명해줘\",\n",
    "    \"answer\" : \"\"\"\n",
    "    나라 : 중국\n",
    "    주요 재료 : 마라, 건두부, 목이버섯\n",
    "    비슷한 음식 : 마라탕\n",
    "    \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector): # BaseExampleSelector를 상속받아서 구현\n",
    "    \n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "        \n",
    "    # def add_example(self,example: Dict[str, str]) -> Any:\n",
    "    #     self.examples.append(example)   \n",
    "    \n",
    "    def add_example(self, example): # 동적으로 예제를 추가할 수 있도록 함\n",
    "        self.examples.append(example) # example은 list 형태 \n",
    "    \n",
    "    def select_examples(self, input_variables): # input_variables는 사용자가 입력한 변수들\n",
    "        # example의 예제 중 하나를 리턴해주도록 함\n",
    "        # 아래 예시는 예제 선택지를 고르는 매우~~ 간단한 예제 -> 심화해서 작성해보기기\n",
    "        from random import choice\n",
    "        return [choice(self.examples)] # 무작위에서 하나만 선택\n",
    "        \n",
    "\n",
    "RandomExampleSelector\n",
    "example_prompts = PromptTemplate.from_template(\"Human : {question} \\n AI:{answer}\") # 답변 형식을 지정해줌\n",
    "\n",
    "# TypeError: Can't instantiate abstract class RandomExampleSelector with abstract method add_example : add _example 메소드를 구현해줘야함\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt =example_prompts, \n",
    "    example_selector = example_selector, # 이전에는 예제를 다 넣어줬지만 이번에는 예제를 선택해줄 수 있음, 원래 exmple을 넣었었음\n",
    "    suffix  = \"Human : {food} 설명해줘\" ,# 뒤에 나올 고정말 \n",
    "    input_variables = ['food'] \n",
    "\n",
    ")\n",
    "\n",
    "prompt.format(food = \"굴보쌈\")\n",
    "\n",
    "# chain = prompt | chat # t사용자의 질문이 LLM 에 전달되어도 예제들이 허용되는 양을 지키면서 LLM에 전달됨\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialization and Composition\n",
    "- 프롬프트를 저장해서 다른 사람과 공유하며 가져다 쓸 수 있도록 하고싶은 경우 활용 (load_prompt)\n",
    "- 두가지 타입의 prompt를 만들거임(json, yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"France\") # prompt에 변수넣고 템플릿을 볼수있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 많은 프롬프트들의 memory 등을 다 모으는 방법\n",
    "   - 많은 프롬프트가 있을때 유용함\n",
    "   - 하나로 다 합쳐야하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \n",
      "    You are a role playing assistant.\n",
      "    And you are impersonating a Pirate\n",
      "\n",
      "                                     \n",
      "    \n",
      "    This is an example of how you talk:\n",
      "\n",
      "    Human: what is your location?\n",
      "    You: Arrrg! That is a secret!\n",
      "\n",
      "                              \n",
      "    \n",
      "    Start now!\n",
      "\n",
      "    Human: What is your fav food?\n",
      "    You:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 4개의 프롬프트를 만들기\n",
    "\n",
    "# 소개용 프롬프트\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 예제용 프롬프트\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 마지막으로 합쳐주는 프롬프트트\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "prompts = [\n",
    "    (\"intro\",intro), # key값은 final 의 변수명과 같아야함\n",
    "    (\"example\",example),\n",
    "    (\"start\",start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(final_prompt=final, pipeline_prompts=prompts)\n",
    "a=full_prompt.format(\n",
    "    character =\"Pirate\",\n",
    "    example_question=\"what is your location?\", # 예시 질문문\n",
    "    example_answer=\"Arrrg! That is a secret!\", # 예시 답변\n",
    "    question=\"What is your fav food?\" # 실제 질문\n",
    "\n",
    ")\n",
    "print(a)\n",
    "# full_prompt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"character\": \"Pirate\",\n",
      "  \"example_question\": \"what is your location?\",\n",
      "  \"example_answer\": \"Arrrg! That is a secret!\",\n",
      "  \"question\": \"What is your fav food?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:PipelinePromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"character\": \"Pirate\",\n",
      "  \"example_question\": \"what is your location?\",\n",
      "  \"example_answer\": \"Arrrg! That is a secret!\",\n",
      "  \"question\": \"What is your fav food?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:PipelinePromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"base\",\n",
      "    \"StringPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"text\": \"\\n    \\n    You are a role playing assistant.\\n    And you are impersonating a Pirate\\n\\n                                     \\n    \\n    This is an example of how you talk:\\n\\n    Human: what is your location?\\n    You: Arrrg! That is a secret!\\n\\n                              \\n    \\n    Start now!\\n\\n    Human: What is your fav food?\\n    You:\\n\\n\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    \\n    You are a role playing assistant.\\n    And you are impersonating a Pirate\\n\\n                                     \\n    \\n    This is an example of how you talk:\\n\\n    Human: what is your location?\\n    You: Arrrg! That is a secret!\\n\\n                              \\n    \\n    Start now!\\n\\n    Human: What is your fav food?\\n    You:\"\n",
      "  ]\n",
      "}\n",
      "Arrrg! Me favorite grub be a hearty bowl o' grog and a side o' salted fish! Nothin' like a good feast after a long day o' plunderin' the high seas! What be yer favorite dish, matey?\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] [1.83s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Arrrg! Me favorite grub be a hearty bowl o' grog and a side o' salted fish! Nothin' like a good feast after a long day o' plunderin' the high seas! What be yer favorite dish, matey?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"example\": false,\n",
      "            \"content\": \"Arrrg! Me favorite grub be a hearty bowl o' grog and a side o' salted fish! Nothin' like a good feast after a long day o' plunderin' the high seas! What be yer favorite dish, matey?\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.83s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"Arrrg! Me favorite grub be a hearty bowl o' grog and a side o' salted fish! Nothin' like a good feast after a long day o' plunderin' the high seas! What be yer favorite dish, matey?\")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 4개의 프롬프트를 만들기\n",
    "\n",
    "# 소개용 프롬프트\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 예제용 프롬프트\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 마지막으로 합쳐주는 프롬프트트\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "prompts = [\n",
    "    (\"intro\",intro), # key값은 final 의 변수명과 같아야함\n",
    "    (\"example\",example),\n",
    "    (\"start\",start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(final_prompt=final, pipeline_prompts=prompts)\n",
    "\n",
    "# chain으로 활용하기\n",
    "chain = full_prompt | chat\n",
    "chain.invoke({\n",
    "    \"character\" :\"Pirate\",\n",
    "    \"example_question\":\"what is your location?\", # 예시 질문문\n",
    "    \"example_answer\":\"Arrrg! That is a secret!\", # 예시 답변\n",
    "    \"question\":\"What is your fav food?\" # 실제 질문\n",
    "    \n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Caching\n",
    "- I/O 모듈 \n",
    "- streamlit에 대해 배우고 랭체인과 streamlit을 써보기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# 챗봇이 있고 그 채팅봇이 항상 똑같은 질문을 받는다면, 답변을 계속 만들지 않고, 이미 답변한 답을 캐싱하여 이용하고 저장하여 재사용할 수 있도록 함\n",
    "# 비용을 아끼는데 효율적임 . 같은 질문을 하면 같은 답변이 나올 것임\n",
    "\n",
    "set_llm_cache(InMemoryCache()) # 캐시를 사용하겠다고 선언 /모든 response를 캐싱하여 메모리에 저장함\n",
    "\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "# 처음 질문때와 두번째 같은 질문할때 시간이 얼마나 걸리는 지 확인\n",
    "\n",
    "# chat.predict(\"이탈리아 파스타는 어떻게 만드나요?\") # 처음 질문 : 11.3초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 이탈리아 파스타는 어떻게 만드나요?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [12.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"이탈리아 파스타는 다양한 종류와 조리법이 있지만, 기본적인 파스타를 만드는 방법을 소개하겠습니다. 여기서는 밀가루와 계란을 사용한 전통적인 이탈리아식 파스타 반죽을 만드는 방법을 설명할게요.\\n\\n### 재료\\n- 밀가루 (00형 또는 일반 밀가루) 100g\\n- 계란 1개\\n- 소금 약간\\n- 올리브 오일 (선택 사항)\\n\\n### 만드는 방법\\n\\n1. **반죽 준비하기**:\\n   - 깨끗한 작업대에 밀가루를 쌓아 원형으로 만들고, 가운데에 계란을 깨서 넣습니다.\\n   - 소금을 약간 넣고, 원형의 밀가루 가장자리를 계란으로 덮어줍니다.\\n\\n2. **반죽하기**:\\n   - 포크를 사용해 계란을 휘저어 밀가루와 섞어줍니다. 점차 밀가루를 계란에 섞어가며 반죽을 만듭니다.\\n   - 반죽이 어느 정도 뭉쳐지면 손으로 반죽을 치대어 부드럽고 탄력 있는 반죽이 될 때까지 약 10분 정도 반죽합니다.\\n\\n3. **휴지시키기**:\\n   - 반죽을 랩으로 싸서 30분 정도 실온에서 휴지시킵니다. 이 과정은 글루텐이 안정화되어 반죽이 더 부드러워지게 합니다.\\n\\n4. **파스타 모양 만들기**:\\n   - 휴지시킨 반죽을 밀대로 얇게 밀어줍니다. 원하는 두께로 밀어준 후, 원하는 형태로 잘라줍니다. (예: 태그리넬리, 파르팔레 등)\\n   - 또는 파스타 기계를 사용해도 좋습니다.\\n\\n5. **파스타 삶기**:\\n   - 큰 냄비에 물을 끓이고 소금을 넣습니다. \\n   - 잘라놓은 파스타를 넣고 2-4분 정도 삶습니다. (파스타의 두께에 따라 다를 수 있습니다.)\\n   - 삶은 후에는 체에 받쳐 물기를 빼고, 원하는 소스와 함께 섞어줍니다.\\n\\n### 소스 추천\\n- **토마토 소스**: 신선한 토마토, 마늘, 올리브 오일, 바질로 만든 소스.\\n- **알프레도 소스**: 크림, 버터, 파마산 치즈로 만든 부드러운 소스.\\n- **페스토 소스**: 바질, 올리브 오일, 파마산 치즈, 잣으로 만든 소스.\\n\\n이렇게 만든 파스타는 신선하고 맛있습니다. 다양한 소스와 함께 즐겨보세요!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"이탈리아 파스타는 다양한 종류와 조리법이 있지만, 기본적인 파스타를 만드는 방법을 소개하겠습니다. 여기서는 밀가루와 계란을 사용한 전통적인 이탈리아식 파스타 반죽을 만드는 방법을 설명할게요.\\n\\n### 재료\\n- 밀가루 (00형 또는 일반 밀가루) 100g\\n- 계란 1개\\n- 소금 약간\\n- 올리브 오일 (선택 사항)\\n\\n### 만드는 방법\\n\\n1. **반죽 준비하기**:\\n   - 깨끗한 작업대에 밀가루를 쌓아 원형으로 만들고, 가운데에 계란을 깨서 넣습니다.\\n   - 소금을 약간 넣고, 원형의 밀가루 가장자리를 계란으로 덮어줍니다.\\n\\n2. **반죽하기**:\\n   - 포크를 사용해 계란을 휘저어 밀가루와 섞어줍니다. 점차 밀가루를 계란에 섞어가며 반죽을 만듭니다.\\n   - 반죽이 어느 정도 뭉쳐지면 손으로 반죽을 치대어 부드럽고 탄력 있는 반죽이 될 때까지 약 10분 정도 반죽합니다.\\n\\n3. **휴지시키기**:\\n   - 반죽을 랩으로 싸서 30분 정도 실온에서 휴지시킵니다. 이 과정은 글루텐이 안정화되어 반죽이 더 부드러워지게 합니다.\\n\\n4. **파스타 모양 만들기**:\\n   - 휴지시킨 반죽을 밀대로 얇게 밀어줍니다. 원하는 두께로 밀어준 후, 원하는 형태로 잘라줍니다. (예: 태그리넬리, 파르팔레 등)\\n   - 또는 파스타 기계를 사용해도 좋습니다.\\n\\n5. **파스타 삶기**:\\n   - 큰 냄비에 물을 끓이고 소금을 넣습니다. \\n   - 잘라놓은 파스타를 넣고 2-4분 정도 삶습니다. (파스타의 두께에 따라 다를 수 있습니다.)\\n   - 삶은 후에는 체에 받쳐 물기를 빼고, 원하는 소스와 함께 섞어줍니다.\\n\\n### 소스 추천\\n- **토마토 소스**: 신선한 토마토, 마늘, 올리브 오일, 바질로 만든 소스.\\n- **알프레도 소스**: 크림, 버터, 파마산 치즈로 만든 부드러운 소스.\\n- **페스토 소스**: 바질, 올리브 오일, 파마산 치즈, 잣으로 만든 소스.\\n\\n이렇게 만든 파스타는 신선하고 맛있습니다. 다양한 소스와 함께 즐겨보세요!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 18,\n",
      "      \"completion_tokens\": 615,\n",
      "      \"total_tokens\": 633,\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"cached_tokens\": 0,\n",
      "        \"audio_tokens\": 0\n",
      "      },\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"system_fingerprint\": \"fp_01aeff40ea\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이탈리아 파스타는 다양한 종류와 조리법이 있지만, 기본적인 파스타를 만드는 방법을 소개하겠습니다. 여기서는 밀가루와 계란을 사용한 전통적인 이탈리아식 파스타 반죽을 만드는 방법을 설명할게요.\\n\\n### 재료\\n- 밀가루 (00형 또는 일반 밀가루) 100g\\n- 계란 1개\\n- 소금 약간\\n- 올리브 오일 (선택 사항)\\n\\n### 만드는 방법\\n\\n1. **반죽 준비하기**:\\n   - 깨끗한 작업대에 밀가루를 쌓아 원형으로 만들고, 가운데에 계란을 깨서 넣습니다.\\n   - 소금을 약간 넣고, 원형의 밀가루 가장자리를 계란으로 덮어줍니다.\\n\\n2. **반죽하기**:\\n   - 포크를 사용해 계란을 휘저어 밀가루와 섞어줍니다. 점차 밀가루를 계란에 섞어가며 반죽을 만듭니다.\\n   - 반죽이 어느 정도 뭉쳐지면 손으로 반죽을 치대어 부드럽고 탄력 있는 반죽이 될 때까지 약 10분 정도 반죽합니다.\\n\\n3. **휴지시키기**:\\n   - 반죽을 랩으로 싸서 30분 정도 실온에서 휴지시킵니다. 이 과정은 글루텐이 안정화되어 반죽이 더 부드러워지게 합니다.\\n\\n4. **파스타 모양 만들기**:\\n   - 휴지시킨 반죽을 밀대로 얇게 밀어줍니다. 원하는 두께로 밀어준 후, 원하는 형태로 잘라줍니다. (예: 태그리넬리, 파르팔레 등)\\n   - 또는 파스타 기계를 사용해도 좋습니다.\\n\\n5. **파스타 삶기**:\\n   - 큰 냄비에 물을 끓이고 소금을 넣습니다. \\n   - 잘라놓은 파스타를 넣고 2-4분 정도 삶습니다. (파스타의 두께에 따라 다를 수 있습니다.)\\n   - 삶은 후에는 체에 받쳐 물기를 빼고, 원하는 소스와 함께 섞어줍니다.\\n\\n### 소스 추천\\n- **토마토 소스**: 신선한 토마토, 마늘, 올리브 오일, 바질로 만든 소스.\\n- **알프레도 소스**: 크림, 버터, 파마산 치즈로 만든 부드러운 소스.\\n- **페스토 소스**: 바질, 올리브 오일, 파마산 치즈, 잣으로 만든 소스.\\n\\n이렇게 만든 파스타는 신선하고 맛있습니다. 다양한 소스와 함께 즐겨보세요!'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"이탈리아 파스타는 어떻게 만드나요?\") # 즉시답변함 / 캐싱을 통해 chat을 이용하지 않고 바로 답변함/ 똑같은 질문을 계속 받아도 답변을 재사용하여 비용을 아낄 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 이탈리아 파스타는 어떻게 만드나요?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [12.65s] Llm run errored with error:\n",
      "\u001b[0m\"KeyboardInterrupt()\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 26\u001b[0m\n\u001b[0;32m     15\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[0;32m     16\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     17\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# ],\u001b[39;00m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 처음 질문때와 두번째 같은 질문할때 시간이 얼마나 걸리는 지 확인\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m이탈리아 파스타는 어떻게 만드나요?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 처음 질문 : 11.3초\u001b[39;00m\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\chat_models\\base.py:636\u001b[0m, in \u001b[0;36mBaseChatModel.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    635\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[1;32m--> 636\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\chat_models\\base.py:600\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    595\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    599\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m--> 600\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\chat_models\\base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    348\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    350\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    351\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    353\u001b[0m ]\n\u001b[0;32m    354\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\chat_models\\base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 339\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m         )\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\chat_models\\base.py:505\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 505\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\chat_models\\openai.py:412\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    411\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 412\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\chat_models\\openai.py:342\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mC:\\Python3116\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mC:\\Python3116\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\chat_models\\openai.py:340\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\openai\\api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupplied_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\openai\\api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabs_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTIMEOUT_SECS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Python3116\\Python311\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Python3116\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python3116\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python3116\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python3116\\Python311\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Python3116\\Python311\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set_debug 이용해보기\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# 챗봇이 있고 그 채팅봇이 항상 똑같은 질문을 받는다면, 답변을 계속 만들지 않고, 이미 답변한 답을 캐싱하여 이용하고 저장하여 재사용할 수 있도록 함\n",
    "# 비용을 아끼는데 효율적임 . 같은 질문을 하면 같은 답변이 나올 것임\n",
    "\n",
    "set_llm_cache(InMemoryCache()) # 캐시를 사용하겠다고 선언 /모든 response를 캐싱하여 메모리에 저장함\n",
    "set_debug(True) # 디버깅을 위한 옵션 / 디버깅을 위한 로그를 출력함\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "# 처음 질문때와 두번째 같은 질문할때 시간이 얼마나 걸리는 지 확인\n",
    "\n",
    "chat.predict(\"이탈리아 파스타는 어떻게 만드나요?\") # 처음 질문 : 11.3초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 이탈리아 파스타는 어떻게 만드나요?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [1ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"이탈리아 파스타를 만드는 방법은 여러 가지가 있지만, 기본적인 파스타 반죽을 만드는 방법을 소개해드릴게요. \\n\\n### 기본 파스타 반죽 재료\\n- 밀가루 (00형 또는 일반 밀가루) 400g\\n- 계란 4개\\n- 소금 1작은술\\n- 올리브 오일 (선택사항) 1큰술\\n\\n### 만드는 방법\\n\\n1. **밀가루 준비**: 깨끗한 작업대나 큰 볼에 밀가루를 쌓아 원형으로 만들고, 가운데에 우물을 만듭니다.\\n\\n2. **계란 추가**: 우물 중앙에 계란을 깨고 소금과 올리브 오일을 추가합니다.\\n\\n3. **반죽하기**: 포크를 사용해 계란을 휘저어가며 밀가루를 조금씩 섞어줍니다. 반죽이 뭉쳐지기 시작하면 손으로 반죽을 치대기 시작합니다.\\n\\n4. **반죽 치대기**: 반죽이 부드럽고 탄력 있게 될 때까지 약 10분 정도 치대줍니다. 필요에 따라 밀가루를 조금 더 추가할 수 있습니다.\\n\\n5. **휴지**: 반죽을 랩으로 싸서 최소 30분간 실온에서 휴지시킵니다. 이 과정은 글루텐이 안정화되어 반죽이 더 부드러워지게 합니다.\\n\\n6. **밀어내기**: 휴지 후 반죽을 원하는 두께로 밀어냅니다. 파스타 기계가 있다면 기계를 사용해도 좋습니다.\\n\\n7. **자르기**: 원하는 형태로 자릅니다. 예를 들어, 파르팔레(나비 모양), 태그리올리니(얇은 면), 라자냐(넓은 면) 등으로 자를 수 있습니다.\\n\\n8. **삶기**: 끓는 소금물에 파스타를 넣고 2-4분 정도 삶습니다. (생 파스타는 일반적으로 빠르게 익습니다.)\\n\\n9. **소스와 함께 제공**: 삶은 파스타를 원하는 소스와 함께 섞어 서빙합니다. 토마토 소스, 알프레도 소스, 바질 페스토 등 다양한 소스를 사용할 수 있습니다.\\n\\n이렇게 기본적인 이탈리아 파스타를 만들 수 있습니다. 다양한 재료와 소스를 활용해 나만의 스타일로 즐겨보세요!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"이탈리아 파스타를 만드는 방법은 여러 가지가 있지만, 기본적인 파스타 반죽을 만드는 방법을 소개해드릴게요. \\n\\n### 기본 파스타 반죽 재료\\n- 밀가루 (00형 또는 일반 밀가루) 400g\\n- 계란 4개\\n- 소금 1작은술\\n- 올리브 오일 (선택사항) 1큰술\\n\\n### 만드는 방법\\n\\n1. **밀가루 준비**: 깨끗한 작업대나 큰 볼에 밀가루를 쌓아 원형으로 만들고, 가운데에 우물을 만듭니다.\\n\\n2. **계란 추가**: 우물 중앙에 계란을 깨고 소금과 올리브 오일을 추가합니다.\\n\\n3. **반죽하기**: 포크를 사용해 계란을 휘저어가며 밀가루를 조금씩 섞어줍니다. 반죽이 뭉쳐지기 시작하면 손으로 반죽을 치대기 시작합니다.\\n\\n4. **반죽 치대기**: 반죽이 부드럽고 탄력 있게 될 때까지 약 10분 정도 치대줍니다. 필요에 따라 밀가루를 조금 더 추가할 수 있습니다.\\n\\n5. **휴지**: 반죽을 랩으로 싸서 최소 30분간 실온에서 휴지시킵니다. 이 과정은 글루텐이 안정화되어 반죽이 더 부드러워지게 합니다.\\n\\n6. **밀어내기**: 휴지 후 반죽을 원하는 두께로 밀어냅니다. 파스타 기계가 있다면 기계를 사용해도 좋습니다.\\n\\n7. **자르기**: 원하는 형태로 자릅니다. 예를 들어, 파르팔레(나비 모양), 태그리올리니(얇은 면), 라자냐(넓은 면) 등으로 자를 수 있습니다.\\n\\n8. **삶기**: 끓는 소금물에 파스타를 넣고 2-4분 정도 삶습니다. (생 파스타는 일반적으로 빠르게 익습니다.)\\n\\n9. **소스와 함께 제공**: 삶은 파스타를 원하는 소스와 함께 섞어 서빙합니다. 토마토 소스, 알프레도 소스, 바질 페스토 등 다양한 소스를 사용할 수 있습니다.\\n\\n이렇게 기본적인 이탈리아 파스타를 만들 수 있습니다. 다양한 재료와 소스를 활용해 나만의 스타일로 즐겨보세요!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이탈리아 파스타를 만드는 방법은 여러 가지가 있지만, 기본적인 파스타 반죽을 만드는 방법을 소개해드릴게요. \\n\\n### 기본 파스타 반죽 재료\\n- 밀가루 (00형 또는 일반 밀가루) 400g\\n- 계란 4개\\n- 소금 1작은술\\n- 올리브 오일 (선택사항) 1큰술\\n\\n### 만드는 방법\\n\\n1. **밀가루 준비**: 깨끗한 작업대나 큰 볼에 밀가루를 쌓아 원형으로 만들고, 가운데에 우물을 만듭니다.\\n\\n2. **계란 추가**: 우물 중앙에 계란을 깨고 소금과 올리브 오일을 추가합니다.\\n\\n3. **반죽하기**: 포크를 사용해 계란을 휘저어가며 밀가루를 조금씩 섞어줍니다. 반죽이 뭉쳐지기 시작하면 손으로 반죽을 치대기 시작합니다.\\n\\n4. **반죽 치대기**: 반죽이 부드럽고 탄력 있게 될 때까지 약 10분 정도 치대줍니다. 필요에 따라 밀가루를 조금 더 추가할 수 있습니다.\\n\\n5. **휴지**: 반죽을 랩으로 싸서 최소 30분간 실온에서 휴지시킵니다. 이 과정은 글루텐이 안정화되어 반죽이 더 부드러워지게 합니다.\\n\\n6. **밀어내기**: 휴지 후 반죽을 원하는 두께로 밀어냅니다. 파스타 기계가 있다면 기계를 사용해도 좋습니다.\\n\\n7. **자르기**: 원하는 형태로 자릅니다. 예를 들어, 파르팔레(나비 모양), 태그리올리니(얇은 면), 라자냐(넓은 면) 등으로 자를 수 있습니다.\\n\\n8. **삶기**: 끓는 소금물에 파스타를 넣고 2-4분 정도 삶습니다. (생 파스타는 일반적으로 빠르게 익습니다.)\\n\\n9. **소스와 함께 제공**: 삶은 파스타를 원하는 소스와 함께 섞어 서빙합니다. 토마토 소스, 알프레도 소스, 바질 페스토 등 다양한 소스를 사용할 수 있습니다.\\n\\n이렇게 기본적인 이탈리아 파스타를 만들 수 있습니다. 다양한 재료와 소스를 활용해 나만의 스타일로 즐겨보세요!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"이탈리아 파스타는 어떻게 만드나요?\") # 캐싱됨됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 김치찌개 어떻게 만드나요?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [7.85s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"김치찌개는 한국의 전통적인 찌개로, 간단하면서도 맛있는 요리입니다. 아래는 기본적인 김치찌개 레시피입니다.\\n\\n### 재료\\n- 잘 익은 김치 2컵\\n- 돼지고기 (목살 또는 삼겹살) 200g\\n- 두부 1/2모\\n- 대파 1대\\n- 양파 1개\\n- 마늘 3~4쪽\\n- 고춧가루 1~2큰술 (취향에 따라 조절)\\n- 국간장 1큰술\\n- 소금, 후추 약간\\n- 물 4컵\\n\\n### 조리 방법\\n1. **재료 손질**: \\n   - 김치는 먹기 좋은 크기로 자릅니다.\\n   - 돼지고기는 한 입 크기로 썰고, 두부는 큐브 모양으로 자릅니다.\\n   - 대파는 어슷하게 썰고, 양파는 채 썰고, 마늘은 다집니다.\\n\\n2. **고기 볶기**: \\n   - 냄비에 돼지고기를 넣고 중불에서 볶아 기름이 나올 때까지 익힙니다.\\n\\n3. **김치 추가**: \\n   - 볶은 고기에 김치를 넣고 함께 볶아 김치가 약간 익을 때까지 볶습니다.\\n\\n4. **물 붓기**: \\n   - 볶은 재료에 물을 붓고 끓입니다.\\n\\n5. **양념하기**: \\n   - 끓기 시작하면 고춧가루, 국간장, 다진 마늘을 넣고 잘 섞습니다.\\n\\n6. **재료 넣기**: \\n   - 양파와 두부를 넣고 중불에서 10~15분 정도 끓입니다. 이때 소금과 후추로 간을 맞춥니다.\\n\\n7. **마무리**: \\n   - 마지막으로 대파를 넣고 2~3분 더 끓인 후 불을 끕니다.\\n\\n8. **서빙**: \\n   - 뜨거운 김치찌개를 그릇에 담아 밥과 함께 즐기세요.\\n\\n맛있게 드세요!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"김치찌개는 한국의 전통적인 찌개로, 간단하면서도 맛있는 요리입니다. 아래는 기본적인 김치찌개 레시피입니다.\\n\\n### 재료\\n- 잘 익은 김치 2컵\\n- 돼지고기 (목살 또는 삼겹살) 200g\\n- 두부 1/2모\\n- 대파 1대\\n- 양파 1개\\n- 마늘 3~4쪽\\n- 고춧가루 1~2큰술 (취향에 따라 조절)\\n- 국간장 1큰술\\n- 소금, 후추 약간\\n- 물 4컵\\n\\n### 조리 방법\\n1. **재료 손질**: \\n   - 김치는 먹기 좋은 크기로 자릅니다.\\n   - 돼지고기는 한 입 크기로 썰고, 두부는 큐브 모양으로 자릅니다.\\n   - 대파는 어슷하게 썰고, 양파는 채 썰고, 마늘은 다집니다.\\n\\n2. **고기 볶기**: \\n   - 냄비에 돼지고기를 넣고 중불에서 볶아 기름이 나올 때까지 익힙니다.\\n\\n3. **김치 추가**: \\n   - 볶은 고기에 김치를 넣고 함께 볶아 김치가 약간 익을 때까지 볶습니다.\\n\\n4. **물 붓기**: \\n   - 볶은 재료에 물을 붓고 끓입니다.\\n\\n5. **양념하기**: \\n   - 끓기 시작하면 고춧가루, 국간장, 다진 마늘을 넣고 잘 섞습니다.\\n\\n6. **재료 넣기**: \\n   - 양파와 두부를 넣고 중불에서 10~15분 정도 끓입니다. 이때 소금과 후추로 간을 맞춥니다.\\n\\n7. **마무리**: \\n   - 마지막으로 대파를 넣고 2~3분 더 끓인 후 불을 끕니다.\\n\\n8. **서빙**: \\n   - 뜨거운 김치찌개를 그릇에 담아 밥과 함께 즐기세요.\\n\\n맛있게 드세요!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 16,\n",
      "      \"completion_tokens\": 493,\n",
      "      \"total_tokens\": 509,\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"cached_tokens\": 0,\n",
      "        \"audio_tokens\": 0\n",
      "      },\n",
      "      \"completion_tokens_details\": {\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini\",\n",
      "    \"system_fingerprint\": \"fp_f2cd28694a\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'김치찌개는 한국의 전통적인 찌개로, 간단하면서도 맛있는 요리입니다. 아래는 기본적인 김치찌개 레시피입니다.\\n\\n### 재료\\n- 잘 익은 김치 2컵\\n- 돼지고기 (목살 또는 삼겹살) 200g\\n- 두부 1/2모\\n- 대파 1대\\n- 양파 1개\\n- 마늘 3~4쪽\\n- 고춧가루 1~2큰술 (취향에 따라 조절)\\n- 국간장 1큰술\\n- 소금, 후추 약간\\n- 물 4컵\\n\\n### 조리 방법\\n1. **재료 손질**: \\n   - 김치는 먹기 좋은 크기로 자릅니다.\\n   - 돼지고기는 한 입 크기로 썰고, 두부는 큐브 모양으로 자릅니다.\\n   - 대파는 어슷하게 썰고, 양파는 채 썰고, 마늘은 다집니다.\\n\\n2. **고기 볶기**: \\n   - 냄비에 돼지고기를 넣고 중불에서 볶아 기름이 나올 때까지 익힙니다.\\n\\n3. **김치 추가**: \\n   - 볶은 고기에 김치를 넣고 함께 볶아 김치가 약간 익을 때까지 볶습니다.\\n\\n4. **물 붓기**: \\n   - 볶은 재료에 물을 붓고 끓입니다.\\n\\n5. **양념하기**: \\n   - 끓기 시작하면 고춧가루, 국간장, 다진 마늘을 넣고 잘 섞습니다.\\n\\n6. **재료 넣기**: \\n   - 양파와 두부를 넣고 중불에서 10~15분 정도 끓입니다. 이때 소금과 후추로 간을 맞춥니다.\\n\\n7. **마무리**: \\n   - 마지막으로 대파를 넣고 2~3분 더 끓인 후 불을 끕니다.\\n\\n8. **서빙**: \\n   - 뜨거운 김치찌개를 그릇에 담아 밥과 함께 즐기세요.\\n\\n맛있게 드세요!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQLiteCache : 결과를 DB에 캐싱하고싶을때 / 메모리에 캐싱하면 서버를 껐다 켜면 사라지므로 DB에 저장함\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# 챗봇이 있고 그 채팅봇이 항상 똑같은 질문을 받는다면, 답변을 계속 만들지 않고, 이미 답변한 답을 캐싱하여 이용하고 저장하여 재사용할 수 있도록 함\n",
    "# 비용을 아끼는데 효율적임 . 같은 질문을 하면 같은 답변이 나올 것임\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\")) # 이름은 뭐든 상관없음(폴더를 보면 자동으로 sqlit db가 생긴걸 확인할 숭 ㅣㅆ음)/ DB에 캐싱하는데 .langchain.db를 사용하게 됨\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "# 처음 질문때와 두번째 같은 질문할때 시간이 얼마나 걸리는 지 확인\n",
    "\n",
    "chat.predict(\"김치찌개 어떻게 만드나요?\") # 처음 질문 : 약 8초초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 김치찌개 어떻게 만드나요?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [2ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"김치찌개는 한국의 전통적인 찌개로, 간단하면서도 맛있는 요리입니다. 아래는 기본적인 김치찌개 레시피입니다.\\n\\n### 재료\\n- 잘 익은 김치 2컵\\n- 돼지고기 (목살 또는 삼겹살) 200g\\n- 두부 1/2모\\n- 대파 1대\\n- 양파 1개\\n- 마늘 3~4쪽\\n- 고춧가루 1~2큰술 (취향에 따라 조절)\\n- 국간장 1큰술\\n- 소금, 후추 약간\\n- 물 4컵\\n\\n### 조리 방법\\n1. **재료 손질**: \\n   - 김치는 먹기 좋은 크기로 자릅니다.\\n   - 돼지고기는 한 입 크기로 썰고, 두부는 큐브 모양으로 자릅니다.\\n   - 대파는 어슷하게 썰고, 양파는 채 썰고, 마늘은 다집니다.\\n\\n2. **고기 볶기**: \\n   - 냄비에 돼지고기를 넣고 중불에서 볶아 기름이 나올 때까지 익힙니다.\\n\\n3. **김치 추가**: \\n   - 볶은 고기에 김치를 넣고 함께 볶아 김치가 약간 익을 때까지 볶습니다.\\n\\n4. **물 붓기**: \\n   - 볶은 재료에 물을 붓고 끓입니다.\\n\\n5. **양념하기**: \\n   - 끓기 시작하면 고춧가루, 국간장, 다진 마늘을 넣고 잘 섞습니다.\\n\\n6. **재료 넣기**: \\n   - 양파와 두부를 넣고 중불에서 10~15분 정도 끓입니다. 이때 소금과 후추로 간을 맞춥니다.\\n\\n7. **마무리**: \\n   - 마지막으로 대파를 넣고 2~3분 더 끓인 후 불을 끕니다.\\n\\n8. **서빙**: \\n   - 뜨거운 김치찌개를 그릇에 담아 밥과 함께 즐기세요.\\n\\n맛있게 드세요!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"김치찌개는 한국의 전통적인 찌개로, 간단하면서도 맛있는 요리입니다. 아래는 기본적인 김치찌개 레시피입니다.\\n\\n### 재료\\n- 잘 익은 김치 2컵\\n- 돼지고기 (목살 또는 삼겹살) 200g\\n- 두부 1/2모\\n- 대파 1대\\n- 양파 1개\\n- 마늘 3~4쪽\\n- 고춧가루 1~2큰술 (취향에 따라 조절)\\n- 국간장 1큰술\\n- 소금, 후추 약간\\n- 물 4컵\\n\\n### 조리 방법\\n1. **재료 손질**: \\n   - 김치는 먹기 좋은 크기로 자릅니다.\\n   - 돼지고기는 한 입 크기로 썰고, 두부는 큐브 모양으로 자릅니다.\\n   - 대파는 어슷하게 썰고, 양파는 채 썰고, 마늘은 다집니다.\\n\\n2. **고기 볶기**: \\n   - 냄비에 돼지고기를 넣고 중불에서 볶아 기름이 나올 때까지 익힙니다.\\n\\n3. **김치 추가**: \\n   - 볶은 고기에 김치를 넣고 함께 볶아 김치가 약간 익을 때까지 볶습니다.\\n\\n4. **물 붓기**: \\n   - 볶은 재료에 물을 붓고 끓입니다.\\n\\n5. **양념하기**: \\n   - 끓기 시작하면 고춧가루, 국간장, 다진 마늘을 넣고 잘 섞습니다.\\n\\n6. **재료 넣기**: \\n   - 양파와 두부를 넣고 중불에서 10~15분 정도 끓입니다. 이때 소금과 후추로 간을 맞춥니다.\\n\\n7. **마무리**: \\n   - 마지막으로 대파를 넣고 2~3분 더 끓인 후 불을 끕니다.\\n\\n8. **서빙**: \\n   - 뜨거운 김치찌개를 그릇에 담아 밥과 함께 즐기세요.\\n\\n맛있게 드세요!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'김치찌개는 한국의 전통적인 찌개로, 간단하면서도 맛있는 요리입니다. 아래는 기본적인 김치찌개 레시피입니다.\\n\\n### 재료\\n- 잘 익은 김치 2컵\\n- 돼지고기 (목살 또는 삼겹살) 200g\\n- 두부 1/2모\\n- 대파 1대\\n- 양파 1개\\n- 마늘 3~4쪽\\n- 고춧가루 1~2큰술 (취향에 따라 조절)\\n- 국간장 1큰술\\n- 소금, 후추 약간\\n- 물 4컵\\n\\n### 조리 방법\\n1. **재료 손질**: \\n   - 김치는 먹기 좋은 크기로 자릅니다.\\n   - 돼지고기는 한 입 크기로 썰고, 두부는 큐브 모양으로 자릅니다.\\n   - 대파는 어슷하게 썰고, 양파는 채 썰고, 마늘은 다집니다.\\n\\n2. **고기 볶기**: \\n   - 냄비에 돼지고기를 넣고 중불에서 볶아 기름이 나올 때까지 익힙니다.\\n\\n3. **김치 추가**: \\n   - 볶은 고기에 김치를 넣고 함께 볶아 김치가 약간 익을 때까지 볶습니다.\\n\\n4. **물 붓기**: \\n   - 볶은 재료에 물을 붓고 끓입니다.\\n\\n5. **양념하기**: \\n   - 끓기 시작하면 고춧가루, 국간장, 다진 마늘을 넣고 잘 섞습니다.\\n\\n6. **재료 넣기**: \\n   - 양파와 두부를 넣고 중불에서 10~15분 정도 끓입니다. 이때 소금과 후추로 간을 맞춥니다.\\n\\n7. **마무리**: \\n   - 마지막으로 대파를 넣고 2~3분 더 끓인 후 불을 끕니다.\\n\\n8. **서빙**: \\n   - 뜨거운 김치찌개를 그릇에 담아 밥과 함께 즐기세요.\\n\\n맛있게 드세요!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"김치찌개 어떻게 만드나요?\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 링크 : https://python.langchain.com/docs/integrations\n",
    "- langchain 문서로 들어가서 integrations 로 들어가면 모든 써드파티 제공업체를 확인할 수 있음\n",
    "- 채팅 모델용 llm을 지원하고, 다른 종류의 loader도 지원함\n",
    "  - text embbeding, retrivate, 캐싱 지원하는 다른 방법 등등\n",
    "\n",
    "- 캐싱관련\n",
    "  - https://python.langchain.com/docs/integrations/llm_caching/\n",
    "  - 다양한 캐싱방법을 지원하므로, 만약 SQLite를 사용하고 싶지 않은 경우, 다른 종류의 캐싱 방법을 사용하거나 직접 개발할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 serialization\n",
    "- openAI 모델 사용시 지불하는 비용 계산하는 방법\n",
    "- 두번째는 모델을 어떻게 저장하고 불러오는지 강의의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zookeeper는 HBase와 같은 분산 시스템에서 주로 사용되지만, Hadoop HDFS 환경에서도 필요할 수 있습니다. HDFS 자체는 Zookeeper 없이도 작동하지만, Zookeeper는 다음과 같은 경우에 유용할 수 있습니다:\n",
      "\n",
      "1. **클러스터 관리**: Zookeeper는 클러스터의 상태를 모니터링하고, 노드의 추가 및 제거를 관리하는 데 도움을 줄 수 있습니다.\n",
      "\n",
      "2. **메타데이터 관리**: HDFS의 메타데이터를 관리하는 데 Zookeeper를 사용할 수 있습니다. 예를 들어, Namenode의 장애 조치(failover) 시 Zookeeper가 유용할 수 있습니다.\n",
      "\n",
      "3. **분산 애플리케이션**: Hadoop 생태계에서 다른 분산 애플리케이션(예: YARN, Kafka 등)을 사용할 경우, Zookeeper가 필요할 수 있습니다.\n",
      "\n",
      "결론적으로, HDFS만 사용하는 경우 Zookeeper는 필수는 아니지만, 클러스터 관리나 장애 조치와 같은 특정 요구 사항이 있을 경우 유용하게 사용할 수 있습니다. Hadoop의 기본 구성에서 NameNode와 DataNode는 HDFS (Hadoop Distributed File System)의 핵심 구성 요소입니다. NameNode는 메타데이터를 관리하고, DataNode는 실제 데이터를 저장합니다. \n",
      "\n",
      "ZooKeeper는 Hadoop의 기본 구성 요소는 아니지만, 특정 상황에서는 유용하게 사용될 수 있습니다. 예를 들어, Hadoop 클러스터의 고가용성을 위해 HA (High Availability) 구성을 설정할 때 ZooKeeper를 사용할 수 있습니다. 이 경우, ZooKeeper는 Active NameNode와 Standby NameNode 간의 상태를 관리하고, 장애 발생 시 자동으로 Active NameNode를 전환하는 역할을 합니다.\n",
      "\n",
      "따라서, 기본적인 Hadoop HDFS 구성에서는 ZooKeeper가 필수는 아니지만, 고가용성 설정을 원할 경우에는 ZooKeeper가 필요합니다. 일반적인 Hadoop 클러스터에서는 NameNode와 DataNode만으로도 운영이 가능하지만, HA 구성을 고려한다면 ZooKeeper를 사용하는 것이 좋습니다. \n",
      "\n",
      "Tokens Used: 495\n",
      "\tPrompt Tokens: 54\n",
      "\tCompletion Tokens: 441\n",
      "Successful Requests: 2\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "set_debug(False)\n",
    "model = \"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a= chat.predict(\"zookeeper는 hbase가 없어도 hadoop hdfs만 구축한 환경에도 필요한거야?\") # 콜백을 사용하여 예측함\n",
    "    b= chat.predict(\"hadoop에 대해 name node, datanode 구성시 zookeeper가 꼭 필요한가?\")\n",
    "    print(a,b,\"\\n\")\n",
    "    print(usage) # 사용량을 출력함 / 그 외, usate.cost, usage.tokens, usage.successful_requests 등을 출력할 수 있음\n",
    "'''\n",
    "Tokens Used: 495\n",
    "\tPrompt Tokens: 54\n",
    "\tCompletion Tokens: 441\n",
    "Successful Requests: 2 # 두번 호출한 횟수 \n",
    "Total Cost (USD): $0.0 # 사용금액\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chat 모델 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "chat = OpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 450\n",
    ")\n",
    "chat.save(\"model.json\") # 모델을 저장함\n",
    "\"\"\"\n",
    "{\n",
    "    \"model_name\": \"gpt-4o-mini\",\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 450,\n",
    "    \"top_p\": 1,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"n\": 1,\n",
    "    \"request_timeout\": null,\n",
    "    \"logit_bias\": {},\n",
    "    \"_type\": \"openai\"\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\llms\\openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\llms\\openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIChat(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4o-mini', model_kwargs={'temperature': 0.1, 'max_tokens': 450, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "chat = load_llm(\"model.json\") # 모델을 불러옴\n",
    "\n",
    "chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
