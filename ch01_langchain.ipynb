{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM basic & LangChain 이해하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic : llm, chat_models 사용해보기\n",
    "- LLM 호출하기 \n",
    "- chat model 사용하기 \n",
    "- langchain은 llm, chat model을 지원해서 두개를 비교해본다\n",
    "\n",
    "- langchain을 이용하면 llm에 대한 모델 api, model 기업등을 알 필요가 없음\n",
    "- 각 모델의 파이선 패키지를 다운로드 받을필요없이 langchain으로 호환되어 연결하여 사용할 수 있음 - api key가 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\llms\\openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\chloeCode\\nomad\\fullstackgpt\\env3116\\Lib\\site-packages\\langchain\\llms\\openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2021년 1월 기준으로 한국의 인구수는 약 5천만 명입니다.',\n",
       " '2023년 기준으로 한국의 인구는 약 5,100만 명 정도입니다. 하지만 인구 수는 지속적으로 변동하므로, 최신 정보를 확인하려면 통계청 등의 공식 자료를 참고하는 것이 좋습니다.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAI # 단일 텍스트 처리\n",
    "from langchain.chat_models import ChatOpenAI # 좀더 현대적인 모델, 대화형 또는 컨텍스트 기반 작업에서 비용효율적적\n",
    "\n",
    "import os\n",
    "\n",
    "'''\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get('OPENAI_API_KEY') # 인식이 안되면 kernel Restart\n",
    "\n",
    "#llm = OpenAI() # The model `text-davinci-003` has been deprecated \n",
    "llm = OpenAI(openai_api_key=api_key, model_name=\"gpt-4o-mini\") # api key를 직접 넣어주는 경우 \n",
    "chat = ChatOpenAI(openai_api_key=api_key, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "'''\n",
    "\n",
    "# model 종류 : gpt-4o-mini, gpt-3.5-turbo, gpt-3.5-turbo-0125, gpt-3.5-turbo-instruct, davinci-002\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-0125\")\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "a = llm.predict(\"한국의 인구수는?\")\n",
    "b = chat.predict(\"한국의 인구수는?\")\n",
    "\n",
    "a,b\n",
    "\n",
    "# OpenAI LLM보다 ChatOpenAI가 더 최신이고 비용이 합리적 -> chatOpenAI 활용 \n",
    "# langchain을 이용하면 여러 llm model들을 호환되는 계층을 사용할 수 있음 (llm, chat 에 모두 predict를 쓴다던가 하는 방식 등)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='VSCode에서 Jupyter 노트북 작업 시 출력값을 자동 줄 바꿈하는 설정을 하려면 다음과 같이 설정할 수 있습니다:\\n\\n1. VSCode에서 Jupyter 노트북을 열고, 출력값이 나타나는 셀을 선택합니다.\\n2. 출력값이 나타나는 셀의 오른쪽 상단에 있는 \"Settings\" 아이콘을 클릭합니다.\\n3. \"Cell Options\" 메뉴에서 \"Wrap\" 옵션을 선택합니다.\\n4. 이제 출력값이 자동으로 줄 바꿈되어 표시될 것입니다.\\n\\n이렇게 설정하면 출력값이 너무 길어서 화면을 넘어가더라도 자동으로 줄 바꿈되어 표시됩니다. 이 설정을 통해 더 편리하게 작업할 수 있을 것입니다.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature = 0.1\n",
    ")\n",
    "\n",
    "# chatOpenAI는 여러 메세지를 입력해서 대화하며 작업할 수 있음\n",
    "message_list = [\n",
    "    SystemMessage(content=\"넌 지금부터 초등학교 선생님이야\"),\n",
    "    AIMessage(content=\"프로그래밍 언어는 파이썬으로 대답해\"),\n",
    "    HumanMessage(content = \"vscode에서 쥬피터노트북 작업할때 출력값을 자동줄바꿈하는 설정하고싶어\")\n",
    "]\n",
    "\n",
    "chat.predict_messages(message_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### template 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPromptTemplate\\n    - string으로 template을 생성\\nChatPromptTemplate \\n    - message로 template을 생성\\n    \\n어떤 설정값을 전달하고 싶지 않다고 했을때, template을 작성\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'한국과 일본 간의 거리는 두 나라의 특정 지점에 따라 다르지만, 가장 가까운 지점인 인천(한국)과 일본의 후쿠오카(후쿠오카현) 사이의 거리는 약 200km 정도입니다. 또한, 서울과 도쿄 간의 직선 거리는 약 1,150km 정도입니다. 두 나라 간의 거리와 관련된 정보는 특정 위치에 따라 달라질 수 있습니다.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The distance between South Korea and Japan varies depending on the specific locations being measured. For example, the distance from Busan, South Korea, to Fukuoka, Japan, is about 200 kilometers (124 miles) across the Korea Strait. If you measure from Seoul to Tokyo, the distance is approximately 1,150 kilometers (715 miles) when considering air travel.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt : llm과 의사소통할 수 있는 유일한 창구로, llm의 성능을 좋게 하기 위해서 전체 산업이 prompt에 집중하고있음\n",
    "# prompt를 만들고 공유하고 결합하고 저장, 불러올 수 있음. 하나의 프롬프트 산업으로 확장댐\n",
    "\n",
    "# 일반 프롬프트 템플릿\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "PromptTemplate\n",
    "    - string으로 template을 생성\n",
    "ChatPromptTemplate \n",
    "    - message로 template을 생성\n",
    "    \n",
    "어떤 설정값을 전달하고 싶지 않다고 했을때, template을 작성\n",
    "'''\n",
    "\n",
    "# model \n",
    "model=\"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(model_name=model, temperature=0.1)\n",
    "\n",
    "template = PromptTemplate.from_template(\"{country_a}와 {country_b}의 거리(km)는?\") # {}을 이용해서 placeholder를 만들어주고 템플릿에서 해당 정보 입력해줄 수 있음음\n",
    "prompt=template.format(country_a=\"한국\", country_b =\"일본\")\n",
    "\n",
    "chat.predict(prompt)\n",
    "\n",
    "#========\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\"넌 지금부터 {who}\"),\n",
    "    (\"ai\", \"답변은 {langs} 해줘\"),\n",
    "    (\"human\",\"{country_a}와 {country_b}의 거리(km)는?\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt=template.format_messages(\n",
    "    who=\"미국인이고 영어로만 답변해야해\",\n",
    "    langs=\"친절하게\",\n",
    "    country_a=\"한국\", country_b =\"일본\"\n",
    ")\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  OutputParser & LangChain Expression language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['서울', '부산', '인천', '대구']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "다양한 llm, template을 호출하고 서로 다른 response을 함께 사용할 수 있음\n",
    "\n",
    "output parser : llm 응답(response)을 변형해야 할 때가 있을때 사용할 수 있음\n",
    "    - 텍스트로만 반환하니까 이걸 DB에 넣거나 dictionary, tuple 타입 등 여러 형태로 변환(transform)하고 싶어질 수 있음\n",
    "'''\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate \n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "model=\"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(model_name=model, temperature=0.1)\n",
    "\n",
    "\n",
    "# 텍스트의 시작 또는 끝에 공백이 있다면 해당 부분을 제거하는 output parser를 생성함\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    \n",
    "    def parse(self, text):\n",
    "        items= text.strip().split(\",\")\n",
    "        return list(map(str.strip,items))\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(\"한국,과,일본,을 영어,로\")\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",'너는 지금부터 리스트 생성 기계다. 입력받은 질문들은 모두 콤마(comma)로 구분된 list로 답해야해. 최대 {max_num}까지만 말해해'),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "\n",
    "prompt=template.format_messages(\n",
    "    max_num = \"4\",\n",
    "    question = \"한국의 광역시는?\"\n",
    ")\n",
    "result = chat.predict_messages(prompt)\n",
    "result # AIMessage(content='서울, 부산, 대구, 인천')\n",
    "\n",
    "p.parse(result.content) # ['서울', '부산', '인천', '대구'] // list로 변환하여 출력함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['서울', '부산', '대구', '인천']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "LangChain Expression Language 사용\n",
    "- message format을 만들고, predict를 호출 -> parser를 다시 호출하는 해당 과정을 쉽게 줄여줄 수 있음\n",
    "\n",
    "# 1단계 : 기본 chain 생성\n",
    "- 모든 요소를 합쳐주는 역할\n",
    "- `|` operation을 이용해서 합쳐진 요소들은 하나의 체인으로 동작할 수 있게 함\n",
    "- format_message와 predict_message(), parser를 호출하는게 내부적으로 동작함 \n",
    "- template, language model, output parser은 있어야함. 해당 요소로 연결함\n",
    "\n",
    "# 2단계 : 활용법 - chain끼리 결합 가능\n",
    "- chian_one 의 결과값을 chain_two의 입력값으로 사용할 수 있음, 추가로 다른 output parser를 사용할 수 있음\n",
    "- 코드 길이을 줄이고 직관적으로 보여줌\n",
    "\n",
    "## 예시\n",
    "chain_one =  template1 | chat1 | CommaOutputParser1() \n",
    "chain_two =  template2 | chat2 | CommaOutputParser2() \n",
    "\n",
    "chain_all = chain_one | chain_two | output\n",
    "chain_all.invoke({...})\n",
    "\n",
    "'''\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate \n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "model=\"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(model_name=model, temperature=0.1)\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    \n",
    "    def parse(self, text):\n",
    "        items= text.strip().split(\",\")\n",
    "        return list(map(str.strip,items))\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",'너는 지금부터 리스트 생성 기계다. 입력받은 질문들은 모두 콤마(comma)로 구분된 list로 답해야해. 최대 {max_num}까지만 말해해'),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "chain = template | chat | CommaOutputParser()  # | 연산자 결합\n",
    "chain.invoke({\n",
    "    \"max_num\" : 4,\n",
    "    \"question\" : \"한국의 광역시는?\"\n",
    "}) # {}이용 및 max값 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining chains\n",
    "- LangChain Expression Language 좀 더 알아보기\n",
    "- 어떻게 동작하는걸까\n",
    "  - 공식문서 및 로직 이해 \n",
    "\n",
    "- LangChain과 친해지고 무엇을 할수있고 어떻게 동작하고 활용할 수 있는지를 이해하는 것을 목적\n",
    "- langchain Component : https://python.langchain.com/v0.2/docs/concepts/\n",
    "  - ![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate \n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "model=\"gpt-4o-mini\"\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model,\n",
    "    temperature=0.1,\n",
    "    streaming = True, # LLM response가 생성되는대로 볼 수 있도록 함 /전체 응답이 끝나기 전에 한글자라도 나오면 볼수있음\n",
    "    callback = [StreamingStdOutCallbackHandler()] # 응답을 콘솔로 바로 print 해줌 / callback : 다양한 이벤트들을 감지할 수 있음(llm 시작 종료 등)\n",
    ")\n",
    "\n",
    "# 레시피를 위한 셰프, 채식주의자를 위한 셰프 두개를 생성\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 지금부터 월드 클래스 셰프야. 너는 내가 원하는 요리에 대한 간단한 레시피를 제공해야해\"),\n",
    "    (\"human\", \"나는 {food}를 요리하고 싶어\")\n",
    "])\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 채식주의자 셰프야. 너는 레시피를 전달받아서 그걸 채식주의자들이 먹을 수 있는 레시피로 변경해줄거야. 기존의 레시피를 너무 많이 벗어나서는 안돼. 잘 모르겠다면 그냥 어떻게 대체할지 모르겠다고 대답해\"),\n",
    "    (\"human\",\"{recipe}\")\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat\n",
    "veg_chef_chian = veg_chef_prompt | chat\n",
    "\n",
    "final_chain = {\"recipe\": chef_prompt} | veg_chef_chian # 이런식으로 넣어주게 되면 앞 체인의 output이 뒷 체인의 recipe로 들어가게 됨\n",
    "'''\n",
    "RunnableMap\n",
    "\n",
    "Langchain이 앞 chef_prompt를 먼저 실행하고, 응답결과를 다음으로 전달함(다음이 무엇인지 상관없음)\n",
    "- 앞에 것이 먼저 실행되고 그 출력값이 다음으로 넘어감\n",
    "'''\n",
    "\n",
    "final_chain.invoke({\"food\" : \"카레\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (참고) langchain을 사용하지 않은 경우 : openAI quickStart 가이드 처럼 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# langcahin을 사용하지 않는 경우, openAI의 api를 쓰기 위해서는 아래처럼 작성이 필요함\n",
    "- 랭체인은 응답을 파싱하고 AIMessage로 변환시켜줌\n",
    "- 랭체인이 output paraer를 동작시켜줌\n",
    "\n",
    "#!pip install openai\n",
    "'''\n",
    "\n",
    "# 참고링크 :  https://platform.openai.com/docs/quickstart?quickstart-example=completions\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku about recursion in programming.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
