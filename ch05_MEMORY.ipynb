{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "- 기본 개요\n",
    "    - 랭체인에는 5가지 정도의 저장방식이 있음\n",
    "    - 각자 사용방법/ 장단점이 다름\n",
    "    - 챗봇에 메모리를 추가하지 않으면 챗봇은 아무것도 기억할 수 없음\n",
    "    - 유저가 자신의 이름을 말하거나 이전 질문에 이어지는 질문을 해도 메모리, 즉 기역력이 없기때문에 대화를 이해할 수 있는 능력이 없음\n",
    "    - openAI에서 제공하는 기본 API는 랭체인없이 사용할 수 있지만 메모리를지원하지 않음\n",
    "    - 즉 어떤 모델에 대화를 걸어더 나중에 다 까먹게됨\n",
    "    - 챗gpt에는 메모리가 존재하고 실제로 어떤 사람과 대화하는 느낌을 가지게함\n",
    "    - 챗봇이 이전의 대화나 질문을 기억하고 답할 수 있기때문임\n",
    "    - 따라서 이번 강의에서는 이런 각 메모리의 종류와 차이점을 보고, 랭체인에 메모리를 탑재하는 방법을 알아봄\n",
    "- 기본 흐름\n",
    "    - 모든 메모리는 save_context, load_memory_variables, input, output 함수를 가지고 있음, 메모리 종류만 다를뿐 API는 동일함\n",
    "- return_message\n",
    "    - 메모리를 우선적으로 만들고, 챗 모델을 위한것 인지 아닌지 선택하고, 챗모델을 위한게 아니라면 return_message=False로 하거나 빼도 됨\n",
    "    - 그럼 history가 문자열로만 표시될 것\n",
    "    - 쳇모델을 사용하고싶다면 True로 바꾸면 챗 모델이 사용할 수 있는 형태로 출력됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 5.1 Conversation Buffer Memory\n",
    "- 단순히 이전 대화내용을 기억하는 메모리\n",
    "- 장점 : 가장 이해하기 쉬운 메모리임\n",
    "- 단점 : 대화내용이 길어질수록 메모리도 커지니까 비효율적임\n",
    "\n",
    "\n",
    "- 모델 자체에는 메모리가 없음\n",
    "  - 우리가 모델에 요청을 보낼때 이전 대화 기록 전체를 같이 보내줘야함\n",
    "  - 그래야 모델이 전에 일어난 대화를 보고 이해할 수 있음\n",
    "  - 유저와 AI 대화가 길어질 수 록 우리가 매번 모델에 보내야할 기록이 길어진것을 의미하며 상당히 비효율적이고 돈이 많이 듬\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi\\nAI: 뭐하니?'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\":\"Hi\"}, {\"output\":\"뭐하니?\"})\n",
    "\n",
    "memory.load_memory_variables({}) # history로 대화한 내용을 불러움\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'), AIMessage(content='뭐하니?')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\":\"Hi\"}, {\"output\":\"뭐하니?\"})\n",
    "\n",
    "memory.load_memory_variables({}) # history로 대화한 내용을 불러움\n",
    "# {'history': [HumanMessage(content='Hi'), AIMessage(content='뭐하니?')]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'),\n",
       "  AIMessage(content='뭐하니?'),\n",
       "  HumanMessage(content='Hi'),\n",
       "  AIMessage(content='뭐하니?')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 메모리는 대화를 저장하고 불러오는 기능을 가지기 떄문에 메모리에 대화내용이 계속 누적시킴\n",
    "\n",
    "memory.save_context({\"input\":\"Hi\"}, {\"output\":\"뭐하니?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'),\n",
       "  AIMessage(content='뭐하니?'),\n",
       "  HumanMessage(content='Hi'),\n",
       "  AIMessage(content='뭐하니?'),\n",
       "  HumanMessage(content='Hi'),\n",
       "  AIMessage(content='뭐하니?')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\":\"Hi\"}, {\"output\":\"뭐하니?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화가 길어질 수 록 수많은 내용이 계속 쌓이기때문에 비효율적이라, 좀 더 효율적으로 메모리, 대화 기록을 최소화하는 메모리를 확인할 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 ConversationBufferWindowMemory\n",
    "- k=4 처럼 메모리 개수를 지정하여 모든 대화를 저장하지 않고 일부만 저장하도록 함\n",
    "- 최신 것만 기억하는 것이 단점이지만 비용이 효율적일 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 대화의 특정 부분만을 저장하는 메모리\n",
    "# 예: 최근 5개만 저장하는 경우, 최근 5개만 저장하고 나머지는 삭제됨\n",
    "# 메모리를 특정 크기로 유지할 수 있고, 모든 대화를 저장하지 않아도 됨\n",
    "# 단점은 챗봇이 최근 대화만 기억함\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True, \n",
    "    k = 4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "    \n",
    "\n",
    "add_message(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)\n",
    "add_message(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})\n",
    "# k=4이기 때문에 1이 삭제됨/ 최신 4개만 저장됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5'),\n",
       "  HumanMessage(content='6'),\n",
       "  AIMessage(content='6')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(6,6)\n",
    "memory.load_memory_variables({}) # 최신 4개만 저장되어 3~6까지만 저장/ 단점은 챗봇이 최근 대화만 기억함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3-1 ConverstaionSummaryMemory\n",
    "- 메세지를 그대로 저장하는것이 아니라 conversation 자체를 요약하는것\n",
    "- llm을 필요로함\n",
    "- 매우 긴 conversation이 있는 경우 유용함\n",
    "- 초반에는 이전보다 많은 토큰고 저장공간을 차지하지만 메세지가 많아질수록 도움이 됨\n",
    "- 요약하는 것이 토큰 양도 줄면서 훨씬 나은 방법이됨\n",
    "- 저장하는 것보다 COnversation을 사용하여 요약하는것이 훨씬 나음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm = llm) \n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"안녕 여기는 서울입니다.\",\"서울의 날씨는 어떠니?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"여기는 광주입니다.\",\"광주가고싶다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'The human greets in Korean and mentions they are in Seoul. The AI asks about the weather in Seoul. The human responds that they are in Gwangju. The AI expresses a desire to visit Gwangju.'}\n"
     ]
    }
   ],
   "source": [
    "print(get_history())\n",
    "\"\"\" 문장을 요약해서 저장해둠/ 내가 입력한 것 보다는 처음에 더 맣은 저장공간을 사용하여 토큰을 사용하지만 대화가 길어질 수록 summary가 각 메세지보다 더 효율적임\n",
    "{'history': 'The human greets in Korean and mentions they are in Seoul. The AI asks about the weather in Seoul. The human responds that they are in Gwangju. The AI expresses a desire to visit Gwangju.'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3-2 ConversationSummaryBufferMemory\n",
    "- Conversation Buffer Memory와 Conversation Summary Memory의 결합임\n",
    "- 메모리에 보내온 메세지 수를 제한\n",
    "- 오래된 메세지들을 요약함\n",
    "- 가장 최근의 상호작용을 계속 추적하며\n",
    "- 가장 최근/ 가장 오래전에 받은 이야기를 모두 추적하고있다는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_tokens = 30, # 최대 토큰 수/ 메세지들이 요약되기 전까지의 토큰 수 \n",
    "    return_messages=True # 채팅모델에 좋음. \n",
    ") \n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human asks the AI to explain ontology. The AI explains that ontology is a knowledge structure design that expresses the concepts (objects) of reality and the relationships between concepts to understand the meaning of data.'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야'),\n",
       "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
       "  AIMessage(content='온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야')]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"온톨로지에 대해 설명해줘.\",\"온톨로지는 현실의 개념(사물)과 개념간의 관계를 표현하여 데이터의 의미를 파악하게 하는 지식의 구조 설계야\")\n",
    "get_history()\n",
    "\n",
    "# max tokens 값이이 30이기 때문에 30을 넘어가면 요약이 되어 저장됨. 아래처럼 SystemMessage로 저장되어 질문과 답변에 대해 요약함\n",
    "\"\"\"\n",
    "'history': [SystemMessage(content='The human asks the AI to explain ontology. The AI explains that ontology is a knowledge structure design that expresses the concepts (objects) of reality and the relationships between concepts to understand the meaning of data.'),\n",
    "  HumanMessage(content='온톨로지에 대해 설명해줘.'),\n",
    "\"\"\"\n",
    "\n",
    "# 최종적으로 system이 내용을 요약하고 저장하고 있으므로, 이건 실제 요금이 청구되는 API를 사용하고 있다는걸 의미함\n",
    "# 따라서, 최대한 효율적으로 사용하는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 ConversationKGMemory\n",
    "- LLM을 사용하는 메모리 클래스\n",
    "- 대화중의 엔티티의 지식 그래프를 생성함\n",
    "- 가장 중요한 것들만 뽑아내는 요약본같은 것\n",
    "- 지식그래프에서 히스토리를 가져오지 않고 엔티티를 가져옴\n",
    "\n",
    "- 참고 : https://python.langchain.com/v0.1/docs/modules/memory/types/kg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On sam: sam is a friend.')]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# chat= ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.1)\n",
    "llm= ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm = llm,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "    v\n",
    "add_message(\"say hi to sam\", \"who is sam\")\n",
    "add_message(\"sam is a friend\", \"okay\")\n",
    "\n",
    "# 출력\n",
    "memory.load_memory_variables({\"input\": \"who is sam\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KnowledgeTriple(subject='sam', predicate='favorite color is', object_='red')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_knowledge_triplets(\"sam's favorite color is red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! I am currently running on a neural network with deep learning capabilities. My primary function is to assist users with information and tasks based on the data provided to me. How can I help you today?'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, verbose=True, prompt=prompt, memory=ConversationKGMemory(llm=llm)\n",
    ")\n",
    "\n",
    "conversation_with_kg.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KnowledgeTriple(subject='sam', predicate='favorite color is', object_='red')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_knowledge_triplets(\"sam's favorite color is red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: My name is James and I'm helping Will. He's an engineer.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello James! It's nice to meet you. How can I assist you with helping Will, the engineer?\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(\n",
    "    input=\"My name is James and I'm helping Will. He's an engineer.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "On Will: Will is an engineer.\n",
      "\n",
      "Conversation:\n",
      "Human: What do you know about Will?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Will is an engineer. He likely has a strong background in math and science, as well as problem-solving skills. He probably works with technology and machinery on a regular basis. Will may also have a detail-oriented and analytical mindset, which are important qualities for an engineer.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input=\"What do you know about Will?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Memory on LLMChain\n",
    "- 메모리를 chain에 꽂는 방법과 두 종류의 chain을 사용해서 꽂는 방법을 배움\n",
    "- llm chain : off the shelf는 일반적인 목적을 가진 체인을 의미함\n",
    "    - 랭체인에 아주 많고 유용함\n",
    "    - 하지만 우리가 스스로 무언가를 만들어볼때 off the shelf chain 보다는 직접 커스텀해서 만든 체인을 만들어서 활용하는 것을 추천\n",
    "    - 빠르게 시작할수있어 좋지만\n",
    "    - 프레임워크를 다루느라 고생하거나 직접 커스텀하고싶을때 이전에 배운것처럼 langchain expression언어를 활용해서 우리의 것을 만들 수 있음\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    You are a helpful AI talking to a human.\n",
      "     \n",
      "    Human: My Name is Chloe   \n",
      "    You : \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello, Chloe! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "# 메모리에는 ConversationSummaryBufferMemory를 사용하고 있음\n",
    "# llmchain에 이 메모리를 넣는 것을 배움\n",
    "# 이게 interaction을 기본적으로 가지고 있는 conversation chain임\n",
    "# 그리고 interaction 토큰 수가 max값보다 크면 가장 오래된 interactiond을 요약해주고 최신내용은 유지해\n",
    "# 그냥 ConversationBufferMemory를 사용하는 것도 상관없음음\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "model_name=\"gpt-4o-mini\",\n",
    "temperature=0.1,\n",
    "tiktoken_model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "# llm= ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.1)\n",
    "# llm= ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120, # 80 이상이면 요약해줌-> 120 변경\n",
    "    memory_key  = \"chat_history\", # 메모리에 저장할 키값/메모리는 chat_history라는 키값을 template에서 찾아서 관련 내용을 찾아줌\n",
    ")\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm = llm,\n",
    "#     memory = memory,\n",
    "#     prompt =PromptTemplate.from_template(\"{question}\"), # LLM에게 question만 주고있음\n",
    "#     verbose = True # 프롬프트 답변을 볼수있는 옵션션\n",
    "# )\n",
    "\n",
    "# hisotry를 알수있게함 - chat_history 이름을 바꿔도 되고, 템플릿에 이전 기록에 뭔가 작업을 하라고 해도 되고 커스텀하면됌. 대신 memory_key와 이름을 맞춰야함함\n",
    "template = \"\"\"\n",
    "\n",
    "    You are a helpful AI talking to a human.\n",
    "    {chat_history} \n",
    "    Human: {question}   \n",
    "    You : \n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    prompt =PromptTemplate.from_template(template), # LLM에게 question만 주고있음\n",
    "    verbose = True # 프롬프트 답변을 볼수있는 옵션션\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My Name is Chloe\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    You are a helpful AI talking to a human.\n",
      "    System: The human introduces herself as Chloe and mentions that she lives in Seoul. The AI greets her and expresses interest in her experience, asking what she enjoys most about living in the vibrant city.\n",
      "Human: What is my name?\n",
      "AI: Your name is Chloe!\n",
      "Human: I live in Seoul\n",
      "AI: That's wonderful, Chloe! Seoul has so much to offer. Do you have any favorite places or activities in the city?\n",
      "Human: I live in Seoul\n",
      "AI: AI: I see you really love to emphasize that! Seoul is indeed a fascinating place. Is there something specific about living in Seoul that you'd like to share or discuss? \n",
      "    Human: I live in Seoul   \n",
      "    You : \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI: It sounds like Seoul is really important to you! What aspects of life in Seoul do you find most enjoyable or unique?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    You are a helpful AI talking to a human.\n",
      "    System: The human introduces herself as Chloe and mentions that she lives in Seoul. The AI greets her and expresses interest in her experience, asking what she enjoys most about living in the vibrant city. The human confirms her name is Chloe and reiterates that she lives in Seoul.\n",
      "AI: That's wonderful, Chloe! Seoul has so much to offer. Do you have any favorite places or activities in the city?\n",
      "Human: I live in Seoul\n",
      "AI: AI: I see you really love to emphasize that! Seoul is indeed a fascinating place. Is there something specific about living in Seoul that you'd like to share or discuss?\n",
      "Human: I live in Seoul\n",
      "AI: AI: It sounds like Seoul is really important to you! What aspects of life in Seoul do you find most enjoyable or unique? \n",
      "    Human: What is my name?   \n",
      "    You : \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Chloe! It's great to chat with you. How has your experience been living in Seoul?\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\") # 대답할수없음답할수없음\n",
    "# 우리가 대화내역을 말해준적이 없어서 기억이 기록되지 않음\n",
    "\n",
    "# 마지막으로 token이 120을 넘어가면 요약해주는 것도 잘 동작함 System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: My Name is Chloe\\nAI: Hello, Chloe! It's nice to meet you. How can I assist you today?\\nHuman: I live in Seoul\\nAI: That's great, Chloe! Seoul is a vibrant city with a rich culture and history. What do you enjoy most about living there?\\nHuman: What is my name?\\nAI: Your name is Chloe!\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # 대화내역을 불러와보기보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Chat Based Memory\n",
    "- 위의 결과는 그저 텍스티일뿐임. 이걸 대화기반의 채팅으로 바꾸는 방식\n",
    "- 대화 기반 메세지 메모리는 매우 쉬움\n",
    "- 문자열 형태 또는 메세지 형태일 수 있음\n",
    "예를들어, 메모리쪽에 저렇게 보면 기록된 프롬프트 방식을 가져올수있는데 채팅기반으로 바꾸려면 문자열이 아니라 메세지로 바꿔달라고 하면됨\n",
    "- 문자열 기반 템플릿대신 chatPromptTemplate을 가져오게 하면됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 사람에게 도움을 주는 AI야\n",
      "Human: My Name is Chloe\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Chloe! How can I assist you today?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate,MessagesPlaceholder\n",
    "# ConversationSummaryBufferMemory가 시스템요약, 사람, ai, 사람, ai 등의 메세지에 기록해나갈때 우리는 그게 얼마나 많은지 알수없기에 MessagesPlaceholder를 사용함\n",
    "# MessagesPlaceholder 역할은 우리가 메세지가 얼마나 많고 누구에게 왔는진 모르지만 메세지 클래스로 대체될 거임\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "model_name=\"gpt-4o-mini\",\n",
    "temperature=0.1,\n",
    "tiktoken_model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120, # 80 이상이면 요약해줌-> 120 변경\n",
    "    memory_key  = \"chat_history\", # 메모리에 저장할 키값/메모리는 chat_history라는 키값을 template에서 찾아서 관련 내용을 찾아줌\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "Prompt= ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 사람에게 도움을 주는 AI야\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # 변수명을 넣어줌 -> 대화요약 메모리는 그 기록을 가져와서 massagePlaceholder에 넣어줌\n",
    "    (\"human\", \"{question}\")\n",
    "    \n",
    "])\n",
    "# MessagesPlaceholder : 사람, ai, 사람, ai 등등 반복될 것들을 넣어줌, 누가보냈는지 알수없는 예츠갛기 어려운 메세지의 양과 제한없는 양의 메세지에 대해 시스템인지 사람, Ai 메세지인지 알수없음음\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    prompt =Prompt,\n",
    "    verbose = True # 프롬프트 답변을 볼수있는 옵션션\n",
    ")\n",
    "\n",
    "\n",
    "chain.predict(question=\"My Name is Chloe\")\n",
    "\n",
    "# 결과 해석하기\n",
    "# system 나오고 Human이 순서대로 나옴, 그 다음 AI - Human 순으로 반복됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 사람에게 도움을 주는 AI야\n",
      "System: The human introduces herself as Chloe, and the AI responds by expressing pleasure in meeting her and asking how it can assist her today. Chloe mentions that she lives in Seoul, and the AI acknowledges this by highlighting Seoul's vibrant culture and history, asking if there's something specific she would like to know or discuss about the city.\n",
      "Human: what is my name?\n",
      "AI: Your name is Chloe!\n",
      "Human: I live in Seoul\n",
      "AI: Yes, you mentioned that you live in Seoul. How do you like living there? Is there anything specific you want to discuss about the city?\n",
      "Human: I live in Seoul\n",
      "AI: Got it! If there's anything specific you'd like to share about living in Seoul or if you have any questions or topics in mind, feel free to let me know!\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I understand! Living in Seoul must be an exciting experience. If there's anything specific you'd like to talk about regarding your life there, such as places to visit, food to try, or cultural experiences, just let me know!\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 사람에게 도움을 주는 AI야\n",
      "System: The human introduces herself as Chloe, and the AI expresses pleasure in meeting her and asks how it can assist her today. Chloe mentions that she lives in Seoul, and the AI acknowledges this by highlighting Seoul's vibrant culture and history, asking if there's something specific she would like to know or discuss about the city. Chloe then asks the AI what her name is, to which the AI responds that her name is Chloe. The AI reiterates her residence in Seoul and inquires how she likes living there and if there's anything specific she wants to discuss about the city.\n",
      "Human: I live in Seoul\n",
      "AI: Got it! If there's anything specific you'd like to share about living in Seoul or if you have any questions or topics in mind, feel free to let me know!\n",
      "Human: I live in Seoul\n",
      "AI: I understand! Living in Seoul must be an exciting experience. If there's anything specific you'd like to talk about regarding your life there, such as places to visit, food to try, or cultural experiences, just let me know!\n",
      "Human: what is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Chloe! It's nice to meet you, Chloe. How can I assist you today?\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [SystemMessage(content=\"The human introduces herself as Chloe, and the AI expresses pleasure in meeting her and asks how it can assist her today. Chloe mentions that she lives in Seoul, and the AI acknowledges this by highlighting Seoul's vibrant culture and history, asking if there's something specific she would like to know or discuss about the city. Chloe then asks the AI what her name is, to which the AI responds that her name is Chloe. The AI reiterates her residence in Seoul and inquires how she likes living there and if there's anything specific she wants to discuss about the city. Chloe confirms her residence in Seoul, and the AI encourages her to share more about her experiences or any questions she may have.\"),\n",
       "  HumanMessage(content='I live in Seoul'),\n",
       "  AIMessage(content=\"I understand! Living in Seoul must be an exciting experience. If there's anything specific you'd like to talk about regarding your life there, such as places to visit, food to try, or cultural experiences, just let me know!\"),\n",
       "  HumanMessage(content='what is my name?'),\n",
       "  AIMessage(content=\"Your name is Chloe! It's nice to meet you, Chloe. How can I assist you today?\")]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # 대화내역을 불러와보기보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 LCEL Based Memory\n",
    "- langchain expression 언어를 이용해서 생성된 체인에 메모리를 추가하는 것은 어렵지 않고실제로 변경작업을 할때 권장되는 방법\n",
    "- 메모리를 추가하는게 LLM체인을 사용하는 것만큼 쉬움\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    tiktoken_model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\", \n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "Prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"너는 사람에게 도움을 주는 AI야\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 방법1 : invoke에서 메모리를 직접 넣어주는 방식식\n",
    "chain = Prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "    \"chat_history\" : memory.load_memory_variables({})[\"chat_history\"],\n",
    "    \"question\" : \"My Name is Chloe\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    tiktoken_model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\", # 기본적으로 메모리키가 history로 설정되어 있어 꼭 chat_history로 안써도 가능(아래 key 명칭을 다 history로 바꾸면 됨됨)\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "Prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"너는 사람에게 도움을 주는 AI야\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 방법2\n",
    "# def load_memory(input): # chain에선 모든 것이 input을 얻고, 그 후 output을 주는 규칙\n",
    "#     print(\"input = \", input)\n",
    "#     return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "def load_memory(_):  # 명시적으로 input이라 하지않아도 _ 해도 문제없음음\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | Prompt | llm\n",
    "# RunnablePassthrough 먼저 실행되며 Prompt에게 history를 전달하고, Prompt는 llm에게 전달함\n",
    "# assign은 변수를 할당하는 것\n",
    "# 해당 함수에서 원하는 어떤 값이던 변수에 할당할 수 있음\n",
    "\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"My Name is Chloe\"  # 우리가 입력한 질문이 chain에서 첫번째 아이템의 input으로 입력됨(RunnablePassthrough~ 이쪽) : 규칙임\n",
    "    }\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "체인에는 메모리가 존재하고, 우리는 load_memory라는 함수를 실행하고, 해당 함수는 history의 키를 사용하는 prompt에 전달되어야함\n",
    "=> load_memory를 실행시키고, 그 함수를 실행시키고 얻은 output을 history라는 속성으로 들어가야함\n",
    "그리고 그게 사용자의 input과 결합되어 프롬프트에 들어갈거고, 프롬프트는 history와 질문이 필요함\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 우리는 메모리 관리를 수동으로하고있어서 모든 대화가 저장될 것임\n",
    "# 코드를 통제하기 위해 코드를 좀 더 드러내서 작성하는 방식으로 진행함함\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, Chloe! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is Chloe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Chloe. How can I help you today, Chloe?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
